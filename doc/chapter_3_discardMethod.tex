\documentclass[Report.tex]{subfiles}
\externaldocument[I-]{chapter_1_introduction}
\externaldocument[M-]{chapter_2_method}
\externaldocument[R-]{chapter_4_result}
\externaldocument[C-]{chapter_5_conclusion}
\externaldocument[RE-]{chapter_6_recognition}

\begin{document}
\chapter{Discarded Method}
\label{sec:Discarded Method}
\section{Description}
This chapter covers the methods tried but not included in the end result, because they showed dissatisfactory results. This chapter is also organized like Chapter~\ref{2-sec:Method}, but for component description please refer to the previous chapter.

\section{Text Segmentation}
\section{Preprocessing}
\subsection{Find rotation}

\begin{flushleft}
  \textbf{Approach: Hough Transform} \\
  \href{https://en.wikipedia.org/wiki/Hough_transform}{Hough transform} is a well known algorithm to find lines, its approach is to see if it can align some threshold of pixels on one straight line. To help with this process we will do a simple edge detection algorithm on the image before running it through the Hough Transform. \par Bellow, the steps needed to perform this approach are mentioned.

  \begin{enumerate}
    \item \textbf{Edge detection}
    Too make the Hough transform perform better we want to remove unnecessary noise. Canny edge detection algorithm is a robust and fast solution for this.
    \item \textbf{Line detection}
    Now perform the actuall Hough Transform.
    \item \textbf{Rotate image}
    Lastly we need to rotate the image according to the lines from the Hough Transform.
  \end{enumerate}
\end{flushleft}

\section{Classification}
\label{sec:Discarded Method:Classification}

\begin{flushleft}
  \textbf{Deep Neural Network} \\
  Multilayer perceptron neural networks are relatively straight
  forward to code, however the challenging part is to decide on good hyper-parameters and to not overfit our network. \par
  Research has shown that the choices of parameters can have huge effects on the error rate through empirical testing. As empirical testing has shown that some combinations of parameters are better than others, we will also use the same method to find decent values on several of the hyper-parameters. More on this bellow, where a short description of the hyper-parameters follow. \par
  One obvious disadvantage we might face by using MLP is that slight spatial change on where in the image the characters are located, might lead to characters classified differently. This is because there is no spatial connections on a MLP, or rather MLP can't connect same object on the image but slightly shifted to right/left as it appears like completely different objects due to the fact MLP is build up.
\end{flushleft}

\begin{figure}[H]
  \centering
  \includegraphics[height=4cm]{res/neural_net2.jpeg}
  \caption{MLP Neural network \href{http://cs231n.github.io/neural-networks-1/}{Source}}
  \label{fig:neural_net2}
\end{figure}


\begin{flushleft}
  \textbf{Hyper-parameters}
  \begin{itemize}
   \item{Number of hidden layers}
   \begin{itemize}
    \item{Layers decide how well the software can define the decision borders. Hence increase in layers can have a positive effect, there are also cons with the amount of layers. The more layers, the greater the computational power needed to train the system. We will be using the empirical method to decide how many layers we need}
   \end{itemize}
   \item{Number of nodes in each hidden layer}
   \begin{itemize}
    \item{Nodes in each hidden layer has the same effect as the number of hidden layers, hence the same applies for this hyper-parameter.}
   \end{itemize}
   \item{Activation functions}
   \begin{itemize}
    \item{The activation function decides which combination of nodes, with their signals, are allowed to propagate through the network. Here we will be using the \textit{rectified linear unit} (RELU) activation function. This is an activation function that allows propagation if the signal is positive, otherwise it will forward a zero. The reason for choosing this activation function is because this function handles the \textit{vanishing gradient problem} better than sigmoid and a tanh activation functions. More on vanishing gradient problem under ``optimization function''.}
   \end{itemize}
   \item{Loss function}
   \begin{itemize}
    \item{The loss function describes how far off the predicted class of the character is from the real class. In our case since we have multiple classes and we are going to use \textit{softmax regression} as the output layer, we also will be using the \textit{cross-entropy loss function}.}
   \end{itemize}
   \item{Optimization function}
   \begin{itemize}
    \item{The backpropogation will train the weights by Gradient Decent Optimization. However as training with several thousand examples, and then optimizing the weights and run the training process, is too costly resource wise, we will have to implement the \textit{mini-batch gradient decent optimization}. Same principle as gradient decent optimization, but this way we will find the gradient decent for each batch. As long as these batches are randomly chosen, and the sizes are large enough, (we will be using 100 as batch size), these will represent the entire dataset well enough.}
   \end{itemize}
   \item{Learning rate}
   \begin{itemize}
    \item{Learning rate is a scalar that decides how large the steps towards the gradient minimum will be, for the weights. Choosing too small of a learning-rate we might risk not reaching the bottom of the graph, we also might get stuck in a local minimum. Choosing too large of a learning rate we might risk never settle down on a minimum. \par
    For the learning rate we will be using the empirical method too.}
   \end{itemize}
   \item{Initialization of the weights and biases}
   \begin{itemize}
    \item{Initialization of the weights also seems to be of importance, researchers have found out. This is obvious, as for example setting all the weights to zero, would of course lead to a network with very few active nodes. \par
    We will be using the initialization of zeros for the biases, not any apparent reason. Based on our research, it seems people have gotten decent results when using this initialization. For the weights we will be using a Gaussian distribution, mean=0, standard deviation=1. Again this is also something that we have read should be a good initialization for the weights, no other reason.}
   \end{itemize}
   \item{Number of epochs}
   \begin{itemize}
    \item{Number of epochs are only relevant when we have a small number of dataset. When we have a small dataset we might want to run the software on the same dataset several times. This might result in overfitting the software to the dataset, therefore it is really important to be careful of the number of epochs, in cases with small datasets.}
   \end{itemize}
  \end{itemize}
\end{flushleft}

\begin{flushleft}
We decided to avoid using MLP in our project as it required lots of training data, is not able to detect spacial or minor rotation changes, e.g. a network trained with MNIST with accuracy over 95\% was doing poorly in recognizing machine-printed characters, even though they were etalon of a perfect input. 

\end{flushleft}

\section{Datasets}
\begin{flushleft}

MNIST was discarded as were decided to stick with machine printed digits and letters, none of which were included in MNIST. It was a good starting point to test our networks (both MLP and CNN) but it was time to move on.

\par
ROT* - dataset containing rotated examples of another set but labels corespont to angle of rotation. In our case we only had 4 classes [0-3], one for each angle in [0, 90, 180, 270].
This set was discarded as out network showed very little learning progress as the accuracy of predictions was constantly around 65\%. Now that is not datasets fault, most likely it was due to bad network topology or poor choice of hyper parameters but we didn't had enough time to debug this part of project and decided to stick to OpenCV's minAreaRect() results. 


\end{flushleft}

\end{document}
